---
config:
  layout: dagre
---
flowchart TD
 subgraph Input["Input"]
        I["Pose Keypoints (x,y coordinates)"]
        EI["Edge Connectivity"]
        BI["Batch Assignment"]
  end
 subgraph subGraph1["Layer 1: GCN"]
        GNN_IN["Node Features [num_nodes, in_channels]"]
        CONV1["GCNConv (in_channels → hidden_channels)"]
        BN1["BatchNorm1d"]
        RELU1["ReLU Activation"]
        DROP1["Dropout (p=0.2)"]
  end
 subgraph subGraph2["Layer 2: GAT with Multi-head Attention"]
        CONV2["GATConv (hidden_channels → hidden_channels/heads)"]
        BN2["BatchNorm1d"]
        RELU2["ReLU Activation"]
        DROP2["Dropout (p=0.2)"]
        ATTHEAD["4 Attention Heads"]
  end
 subgraph subGraph3["GIN Internal MLP"]
        GINL1["Linear Layer (hidden_channels → hidden_channels*2)"]
        GINBN["BatchNorm1d"]
        GINRELU["ReLU Activation"]
        GINL2["Linear Layer (hidden_channels*2 → hidden_channels)"]
  end
 subgraph subGraph4["Layer 3: GIN with MLP"]
        subGraph3
        CONV3["GINConv"]
        BN3["BatchNorm1d"]
        RELU3["ReLU Activation"]
        RESCONN["Residual Connection from Layer 1"]
  end
 subgraph subGraph5["Multi-level Feature Aggregation"]
        JK@{ label: "JumpingKnowledge ('cat' mode)" }
  end
 subgraph subGraph6["Multi-scale Pooling"]
        MP["Mean Pool"]
        MXP["Max Pool"]
        AP["Add Pool"]
        AVGP["Average of 3 Pooling Methods"]
  end
 subgraph PoseGNN["PoseGNN"]
        subGraph1
        subGraph2
        subGraph4
        subGraph5
        subGraph6
        PROJ["Linear Projection (output_dim → hidden_channels)"]
        GNN_OUT["Graph Embeddings [batch_size, hidden_channels]"]
  end
 subgraph subGraph8["Self-Attention Mechanism"]
        MHA["Multi-head Attention (4 heads)"]
        ATTNLN["Layer Norm"]
        ATTNDROP["Dropout"]
  end
 subgraph subGraph9["Feed Forward Network"]
        FFN1["Linear (hidden_channels → hidden_channels*4)"]
        FFNRELU["ReLU Activation"]
        FFN2["Linear (hidden_channels*4 → hidden_channels)"]
        FFNLN["Layer Norm"]
        FFNDROP["Dropout"]
  end
 subgraph subGraph10["Transformer Layers (x2)"]
        TRL1["Transformer Encoder Layer 1"]
        subGraph8
        subGraph9
        TRL2["Transformer Encoder Layer 2"]
  end
 subgraph TransformerEncoder["TransformerEncoder"]
        TR_IN["Transformer Input [batch_size, hidden_channels]"]
        UNSQ1["Unsqueeze to [batch_size, 1, hidden_channels]"]
        POS_EMB["Positional Embedding"]
        ADD1["Add"]
        subGraph10
        SQZ["Squeeze to [batch_size, hidden_channels]"]
        TR_OUT["Transformer Output [batch_size, hidden_channels]"]
  end
 subgraph subGraph12["Classification Head"]
        FC1["Linear Layer (hidden_channels → hidden_channels/2)"]
        RELU4["ReLU Activation"]
        DROP3["Dropout (p=0.3)"]
        FC2["Linear Layer (hidden_channels/2 → 1)"]
        SIGMOID["Sigmoid Activation"]
        CL_OUT["Violence Score [batch_size, 1]"]
  end
    I --> GNN_IN
    EI --> GNN_IN
    BI --> GNN_IN
    GNN_IN --> CONV1
    CONV1 --> BN1
    BN1 --> RELU1
    RELU1 --> DROP1
    DROP1 --> CONV2 & RESCONN & JK
    ATTHEAD --> CONV2
    CONV2 --> BN2
    BN2 --> RELU2
    RELU2 --> DROP2
    DROP2 --> CONV3 & JK
    RESCONN --> CONV3
    GINL1 --> GINBN
    GINBN --> GINRELU
    GINRELU --> GINL2
    GINL2 --> CONV3
    CONV3 --> BN3
    BN3 --> RELU3
    RELU3 --> JK
    JK --> MP & MXP & AP
    MP --> AVGP
    MXP --> AVGP
    AP --> AVGP
    AVGP --> PROJ
    PROJ --> GNN_OUT
    GNN_OUT --> TR_IN
    TR_IN --> UNSQ1
    UNSQ1 --> ADD1
    POS_EMB --> ADD1
    ADD1 --> TRL1
    TRL1 --> TRL2
    TRL2 --> SQZ
    SQZ --> TR_OUT
    MHA --> ATTNLN
    ATTNLN --> ATTNDROP
    ATTNDROP --> TRL1
    FFN1 --> FFNRELU
    FFNRELU --> FFN2
    FFN2 --> FFNLN
    FFNLN --> FFNDROP
    FFNDROP --> TRL1
    TR_OUT --> FC1
    FC1 --> RELU4
    RELU4 --> DROP3
    DROP3 --> FC2
    FC2 --> SIGMOID
    SIGMOID --> CL_OUT
    JK@{ shape: rect}
